import requests
import base64
import os
import json
import re
import yaml
from urllib.parse import urlparse, parse_qs, unquote
import hashlib
import socket
import time
import concurrent.futures

# --- ğŸ¯ é…ç½®å¸¸é‡ ---
# ç›´æ¥å°†é“¾æ¥ç¡¬ç¼–ç åˆ°è„šæœ¬ä¸­
SUBSCRIPTION_URL = "https://raw.githubusercontent.com/qjlxg/pin/refs/heads/main/trojan_links.txt"
OUTPUT_YAML_FILE = "base64.yaml"
OUTPUT_BASE64_FILE = "base64.txt"

MAX_WORKERS_CONNECTIVITY_TEST = 30
EXCLUDE_KEYWORDS = [
    "cdn.jsdelivr.net", "statically.io", "googletagmanager.com",
    "www.w3.org", "fonts.googleapis.com", "schemes.ogf.org", "clashsub.net",
    "t.me", "api.w.org",
]

# --- Proxy Parsing Functions (ä¿æŒä¸å˜) ---
def generate_proxy_fingerprint(proxy_data):
    """
    æ ¹æ®ä»£ç†çš„æ ¸å¿ƒè¿æ¥ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„å“ˆå¸ŒæŒ‡çº¹ã€‚
    """
    try:
        p_type = str(proxy_data.get('type', '')).lower()
        server = str(proxy_data.get('server', ''))
        port = str(proxy_data.get('port', ''))

        fingerprint_parts = [p_type, server, port]

        if p_type == 'vmess':
            fingerprint_parts.append(str(proxy_data.get('uuid', '')))
            fingerprint_parts.append(str(proxy_data.get('alterId', '')))
        elif p_type == 'trojan':
            fingerprint_parts.append(str(proxy_data.get('password', '')))
        elif p_type == 'ss':
            fingerprint_parts.append(str(proxy_data.get('password', '')))
            fingerprint_parts.append(str(proxy_data.get('cipher', '')))
        elif p_type == 'hysteria2':
            fingerprint_parts.append(str(proxy_data.get('password', '')))

        unique_string = "_".join(fingerprint_parts)
        return hashlib.md5(unique_string.encode('utf-8')).hexdigest()
    except Exception as e:
        # print(f"Error generating fingerprint for proxy: {proxy_data}, reason: {e}")
        return None

def parse_vmess(vmess_url):
    try:
        json_str = base64.b64decode(vmess_url[8:]).decode('utf-8')
        config = json.loads(json_str)

        name = config.get('ps', f"Vmess-{config.get('add')}")
        server = config.get('add')
        port = config.get('port')
        uuid = config.get('id')
        alterId = config.get('aid', 0)
        cipher = config.get('scy', 'auto')
        network = config.get('net', 'tcp')
        tls = config.get('tls', '') == 'tls'
        servername = config.get('sni', config.get('host', '')) if tls else ''
        skip_cert_verify = config.get('skip-cert-verify', False)

        proxy = {
            'name': name,
            'type': 'vmess',
            'server': server,
            'port': port,
            'uuid': uuid,
            'alterId': alterId,
            'cipher': cipher,
            'network': network,
            'tls': tls,
        }

        if servername:
            proxy['servername'] = servername
        if skip_cert_verify:
            proxy['skip-cert-verify'] = True

        return proxy
    except Exception as e:
        # print(f"è§£æ Vmess é“¾æ¥å¤±è´¥: {vmess_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

def parse_trojan(trojan_url):
    try:
        parsed = urlparse(trojan_url)
        password = parsed.username
        server = parsed.hostname
        port = parsed.port
        name = unquote(parsed.fragment) if parsed.fragment else f"Trojan-{server}"

        params = parse_qs(parsed.query)
        tls = True
        skip_cert_verify = params.get('allowInsecure', ['0'])[0] == '1'
        servername = params.get('sni', [server])[0]

        proxy = {
            'name': name,
            'type': 'trojan',
            'server': server,
            'port': port,
            'password': password,
            'tls': tls,
        }
        if servername:
            proxy['servername'] = servername
        if skip_cert_verify:
            proxy['skip-cert-verify'] = True

        return proxy
    except Exception as e:
        # print(f"è§£æ Trojan é“¾æ¥å¤±è´¥: {trojan_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

def parse_shadowsocks(ss_url):
    try:
        encoded_part = ss_url[5:]
        name = "Shadowsocks"
        plugin_info_str = ""

        if '#' in encoded_part:
            encoded_part, fragment = encoded_part.split('#', 1)
            name = unquote(fragment)

        if '/?plugin=' in encoded_part:
            encoded_part, plugin_info_str = encoded_part.split('/?plugin=', 1)
            plugin_info_str = unquote(plugin_info_str)

        missing_padding = len(encoded_part) % 4
        if missing_padding:
            encoded_part += '=' * (4 - missing_padding)

        try:
            decoded_bytes = base64.urlsafe_b64decode(encoded_part)
            try:
                decoded_str = decoded_bytes.decode('utf-8')
            except UnicodeDecodeError:
                decoded_str = decoded_bytes.decode('latin-1', errors='ignore')
                # print(f"Â  Â  Warning: Shadowsocks link decoded to non-UTF-8 characters, using latin-1 for {ss_url[:50]}...")

            parts = decoded_str.split('@', 1)
            if len(parts) != 2:
                raise ValueError(f"Invalid format after base64 decoding: Missing '@' separator or incorrect structure.")

            method_password = parts[0]
            method_password_parts = method_password.split(':', 1)
            if len(method_password_parts) != 2:
                raise ValueError(f"Invalid method:password format: '{method_password}'")
            method = method_password_parts[0]
            password = method_password_parts[1]

            server_port_and_tail = parts[1]
            clean_server_port_match = re.match(r'^[\w\d\.\-]+\:(\d+)', server_port_and_tail)
            if clean_server_port_match:
                server = server_port_and_tail.split(':')[0]
                port = int(clean_server_port_match.group(1))
            else:
                raise ValueError(f"Invalid server:port format in: '{server_port_and_tail}'")

            proxy = {
                'name': name,
                'type': 'ss',
                'server': server,
                'port': port,
                'cipher': method,
                'password': password,
            }
            if plugin_info_str:
                proxy['plugin-info'] = plugin_info_str
            return proxy
        except base64.binascii.Error as b64_err:
            raise ValueError(f"Base64 decoding error: {b64_err}")
    except Exception as e:
        # print(f"è§£æ Shadowsocks é“¾æ¥å¤±è´¥: {ss_url[:100]}...ï¼ŒåŸå› : {e}")
        return None

def parse_hysteria2(hy2_url):
    try:
        parsed = urlparse(hy2_url)
        uuid = parsed.username
        server = parsed.hostname
        port = parsed.port
        name = unquote(parsed.fragment) if parsed.fragment else f"Hysteria2-{server}"

        params = parse_qs(parsed.query)
        tls = params.get('security', [''])[0].lower() == 'tls'
        servername = params.get('sni', [''])[0]
        skip_cert_verify = params.get('insecure', ['0'])[0] == '1'
        fast_open = params.get('fastopen', ['0'])[0] == '1'

        proxy = {
            'name': name,
            'type': 'hysteria2',
            'server': server,
            'port': port,
            'password': uuid,
            'tls': tls,
            'skip-cert-verify': skip_cert_verify,
            'fast-open': fast_open,
        }
        if servername:
            proxy['servername'] = servername
        if params.get('alpn'):
            proxy['alpn'] = ','.join(params['alpn'])
        return proxy
    except Exception as e:
        # print(f"è§£æ Hysteria2 é“¾æ¥å¤±è´¥: {hy2_url[:50]}...ï¼ŒåŸå› : {e}")
        return None
# --- Connectivity Test Function (ä¿æŒä¸å˜) ---
def test_tcp_connectivity(server, port, timeout=1, retries=1, delay=0.5):
    for i in range(retries + 1):
        try:
            sock = socket.create_connection((server, int(port)), timeout=timeout)
            sock.close()
            return True
        except (socket.timeout, ConnectionRefusedError, OSError, ValueError) as e:
            if i < retries:
                time.sleep(delay)
        except Exception as e:
            # print(f"Debug: Unexpected error during TCP connect to {server}:{port}: {e}")
            return False
    return False

# --- Subscription Parsing Helper Functions (ä¿æŒä¸å˜) ---
def _parse_single_proxy_link(line):
    """Helper function to parse a single proxy link string."""
    line = line.strip()
    if line.startswith("vmess://"):
        return parse_vmess(line)
    elif line.startswith("trojan://"):
        return parse_trojan(line)
    elif line.startswith("ss://"):
        return parse_shadowsocks(line)
    elif line.startswith("hysteria2://"):
        return parse_hysteria2(line)
    return None

def _try_parse_yaml_proxies(text):
    try:
        data = yaml.safe_load(text)
        if isinstance(data, dict) and 'proxies' in data and isinstance(data['proxies'], list):
            return data['proxies']
        elif isinstance(data, list) and all(isinstance(item, dict) and 'type' in item for item in data):
            return data
        return None
    except yaml.YAMLError:
        return None

def _try_parse_v2rayn_json_proxies(text):
    try:
        data = json.loads(text)
        if isinstance(data, list) and all(isinstance(item, dict) and 'v' in item and 'ps' in item for item in data):
            parsed_list = []
            for node in data:
                vmess_link = f"vmess://{base64.b64encode(json.dumps(node).encode('utf-8')).decode('utf-8')}"
                p = parse_vmess(vmess_link)
                if p:
                    parsed_list.append(p)
            return parsed_list
        return None
    except json.JSONDecodeError:
        return None

def _parse_proxies_from_decoded_text(decoded_text, url_for_logging):
    """
    Tries to parse proxies from decoded text content.
    Attempts YAML, then V2RayN JSON, then line-by-line.
    Returns a list of parsed proxy dicts, or an empty list if none found.
    """
    proxies = []

    yaml_proxies = _try_parse_yaml_proxies(decoded_text)
    if yaml_proxies:
        print(f"  --- URL: {url_for_logging} Identified as YAML subscription, found {len(yaml_proxies)} proxies ---")
        return yaml_proxies

    json_node_proxies = _try_parse_v2rayn_json_proxies(decoded_text)
    if json_node_proxies:
        print(f"  --- URL: {url_for_logging} Identified as V2RayN JSON node list, found {len(json_node_proxies)} proxies ---")
        return json_node_proxies

    lines = decoded_text.split('\n')
    parsed_line_count = 0
    for line in lines:
        proxy = _parse_single_proxy_link(line)
        if proxy:
            proxies.append(proxy)
            parsed_line_count += 1

    if parsed_line_count > 0:
        print(f"  --- URL: {url_for_logging} Identified as plaintext, {parsed_line_count} proxy nodes parsed ---")
    return proxies

# --- Fetch and Decode URLs (ä¸»è¦ä¿®æ”¹æ­¤å‡½æ•°) ---
def fetch_and_decode_urls_to_clash_proxies(urls, enable_connectivity_test=True):
    all_raw_proxies = []
    
    # ---------------------------------------------
    # æ ¸å¿ƒä¿®æ”¹ï¼šåªå¤„ç†è¾“å…¥çš„ URL åˆ—è¡¨ï¼Œä¸å†å¤„ç†æ–‡ä»¶æ›´æ–°é€»è¾‘
    # ---------------------------------------------

    for url_idx, url in enumerate(urls):
        url = url.strip()
        if not url:
            continue

        if any(keyword in url for keyword in EXCLUDE_KEYWORDS):
            print(f"Skipping non-subscription link (filtered by keyword): {url}")
            continue

        print(f"Processing URL ({url_idx + 1}/{len(urls)}): {url}")
        current_proxies_from_url = []

        try:
            response = requests.get(url, timeout=20)
            response.raise_for_status()
            content = response.content
            print(f"  --- URL: {url} Downloaded content size: {len(content)} bytes ---")

            decoded_successfully = False
            try:
                decoded_content_utf8 = content.decode('utf-8')
                decoded_successfully = True

                proxies_from_utf8 = _parse_proxies_from_decoded_text(decoded_content_utf8, url)
                if proxies_from_utf8:
                    current_proxies_from_url.extend(proxies_from_utf8)
                else:
                    stripped_content = decoded_content_utf8.strip()
                    if len(stripped_content) > 0 and (len(stripped_content) % 4 == 0 or '=' not in stripped_content) and re.fullmatch(r'[A-Za-z0-9+/=]*', stripped_content):
                        try:
                            missing_padding = len(stripped_content) % 4
                            if missing_padding:
                                stripped_content += '=' * (4 - missing_padding)

                            decoded_from_base64_in_utf8 = base64.b64decode(stripped_content).decode('utf-8')
                            print(f"  --- URL: {url} Content (originally UTF-8) was Base64, re-decoding and parsing ---")
                            proxies_from_b64_in_utf8 = _parse_proxies_from_decoded_text(decoded_from_base64_in_utf8, url)
                            if proxies_from_b64_in_utf8:
                                current_proxies_from_url.extend(proxies_from_b64_in_utf8)
                        except (base64.binascii.Error, UnicodeDecodeError) as e_b64_utf8:
                            # print(f"  --- URL: {url} Looked like Base64 (in UTF-8 text) but failed to decode/parse: {e_b64_utf8} ---")
                            pass
                        except Exception as e_generic_b64_utf8:
                            # print(f"  --- URL: {url} Unexpected error during Base64 (in UTF-8 text) processing: {e_generic_b64_utf8} ---")
                            pass
            except UnicodeDecodeError:
                # print(f"  --- URL: {url} UTF-8 decoding failed. Will try direct Base64. ---")
                pass

            if not current_proxies_from_url:
                try:
                    cleaned_byte_content = content.strip()
                    try:
                        b64_text_equivalent = cleaned_byte_content.decode('ascii')
                    except UnicodeDecodeError:
                        b64_text_equivalent = cleaned_byte_content.decode('latin-1', errors='ignore')

                    missing_padding = len(b64_text_equivalent) % 4
                    if missing_padding:
                        cleaned_byte_content += b'=' * (4 - missing_padding)

                    decoded_content_b64 = base64.b64decode(cleaned_byte_content).decode('utf-8')
                    proxies_from_b64 = _parse_proxies_from_decoded_text(decoded_content_b64, url)
                    if proxies_from_b64:
                        current_proxies_from_url.extend(proxies_from_b64)
                except (base64.binascii.Error, UnicodeDecodeError) as b64_err:
                    if not decoded_successfully:
                        # print(f"  --- URL: {url} Direct Base64 decoding or subsequent UTF-8 conversion failed: {b64_err} ---")
                        pass
                except Exception as e_b64_direct:
                    # print(f"  --- URL: {url} Unexpected error during direct Base64 processing: {e_b64_direct} ---")
                    pass

            if current_proxies_from_url:
                all_raw_proxies.extend(current_proxies_from_url)
                print(f"  +++ URL: {url} Successfully parsed {len(current_proxies_from_url)} proxies. +++")
            else:
                content_snippet = content[:100].decode('latin-1', errors='ignore')
                print(f"  --- URL: {url} No proxies successfully parsed from this URL. Content snippet (latin-1, first 100 chars): '{content_snippet}' ---")

        except requests.exceptions.RequestException as e:
            print(f"Failed to fetch data from URL: {url}, reason: {e}")
        except Exception as e:
            print(f"An unexpected error occurred while processing URL {url}: {e}")

    # --- Deduplication and Connectivity Test (Parallelized) ---
    unique_proxies_for_test = {}
    for proxy_dict in all_raw_proxies:
        if not proxy_dict or not isinstance(proxy_dict, dict) or 'server' not in proxy_dict or 'port' not in proxy_dict:
            # print(f"Warning: Skipping invalid proxy data: {proxy_dict}")
            continue
        fingerprint = generate_proxy_fingerprint(proxy_dict)
        if fingerprint:
            unique_proxies_for_test[fingerprint] = proxy_dict

    proxies_to_test_list = list(unique_proxies_for_test.values())
    final_filtered_proxies = []

    if enable_connectivity_test:
        print(f"\nå¼€å§‹å¹¶è¡Œè¿é€šæ€§æµ‹è¯•ï¼Œå…± {len(proxies_to_test_list)} ä¸ªå”¯ä¸€ä»£ç†...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_CONNECTIVITY_TEST) as executor:
            future_to_proxy = {
                executor.submit(test_tcp_connectivity, p['server'], p['port']): p
                for p in proxies_to_test_list if p.get('server') and p.get('port') is not None
            }
            processed_count = 0
            total_testable_proxies = len(future_to_proxy)

            for future in concurrent.futures.as_completed(future_to_proxy):
                proxy_dict = future_to_proxy[future]
                server = proxy_dict.get('server')
                port = proxy_dict.get('port')
                processed_count += 1
                try:
                    is_reachable = future.result()
                    if is_reachable:
                        original_name = proxy_dict.get('name', f"{proxy_dict.get('type', 'UNKNOWN').upper()}-{proxy_dict.get('server', 'unknown')}")
                        short_fingerprint = generate_proxy_fingerprint(proxy_dict)[:6]
                        max_name_len = 50

                        if len(original_name) > max_name_len - (len(short_fingerprint) + 1):
                            display_name = original_name[:max_name_len - (len(short_fingerprint) + 4)] + "..."
                        else:
                            display_name = original_name

                        proxy_dict['name'] = f"{display_name}-{short_fingerprint}"
                        final_filtered_proxies.append(proxy_dict)
                except Exception as exc:
                    # print(f"  è¿é€šæ€§æµ‹è¯• {server}:{port} æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                    pass

                if processed_count % 50 == 0 or processed_count == total_testable_proxies:
                    print(f"    è¿›åº¦: å·²æµ‹è¯• {processed_count}/{total_testable_proxies} ä¸ªä»£ç†...")
    else:
        print("è·³è¿‡è¿é€šæ€§æµ‹è¯• (å·²ç¦ç”¨)ã€‚æ‰€æœ‰è§£æå‡ºçš„å”¯ä¸€ä»£ç†å°†è¢«æ·»åŠ ã€‚")
        for proxy_dict in proxies_to_test_list:
            original_name = proxy_dict.get('name', f"{proxy_dict.get('type', 'UNKNOWN').upper()}-{proxy_dict.get('server', 'unknown')}")
            short_fingerprint = generate_proxy_fingerprint(proxy_dict)[:6]
            max_name_len = 50

            if len(original_name) > max_name_len - (len(short_fingerprint) + 1):
                display_name = original_name[:max_name_len - (len(short_fingerprint) + 4)] + "..."
            else:
                display_name = original_name

            proxy_dict['name'] = f"{display_name}-{short_fingerprint}"
            final_filtered_proxies.append(proxy_dict)

    print(f"Successfully parsed, deduplicated, tested, and aggregated {len(final_filtered_proxies)} unique and reachable proxy nodes.")
    # è¿”å›æˆåŠŸçš„ä»£ç†åˆ—è¡¨å’Œç©ºçš„ URL åˆ—è¡¨ï¼ˆå› ä¸ºæˆ‘ä»¬ä¸å†æ›´æ–° URL æ–‡ä»¶ï¼‰
    return final_filtered_proxies, [] 

# --- GitHub API Helpers (ç§»é™¤æˆ–ç®€åŒ–) ---
# ç§»é™¤æ‰€æœ‰ GitHub API ç›¸å…³çš„å‡½æ•° get_github_file_content å’Œ update_github_file_content

# --- Main Function (ä¸»è¦ä¿®æ”¹) ---
def main():
    # ç§»é™¤ BOT_TOKEN å’Œ URL_LIST_REPO_API çš„ç¯å¢ƒå˜é‡è·å–
    # bot_token = os.environ.get("BOT")
    # url_list_repo_api = os.environ.get("URL_LIST_REPO_API")

    # ç›´æ¥ä½¿ç”¨ç¡¬ç¼–ç çš„ URL
    urls = [SUBSCRIPTION_URL]
    print(f"ä½¿ç”¨ç¡¬ç¼–ç çš„è®¢é˜…é“¾æ¥: {SUBSCRIPTION_URL}")

    enable_connectivity_test = os.environ.get("ENABLE_CONNECTIVITY_TEST", "true").lower() == "true"

    # fetch_and_decode_urls_to_clash_proxies å‡½æ•°ç°åœ¨åªè¿”å›ä»£ç†åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªè¿”å›å€¼æ˜¯ç©ºçš„
    all_parsed_proxies, _ = fetch_and_decode_urls_to_clash_proxies(urls, enable_connectivity_test)

    clash_config = {
        'port': 7890,
        'socks-port': 7891,
        'redir-port': 7892,
        'tproxy-port': 7893,
        'mixed-port': 7890,
        'mode': 'rule',
        'log-level': 'info',
        'allow-lan': True,
        'bind-address': '*',
        'external-controller': '127.0.0.1:9090',
        'dns': {
            'enable': True,
            'ipv6': False,
            'enhanced-mode': 'fake-ip',
            'listen': '0.0.0.0:53',
            'default-nameserver': ['114.114.114.114', '8.8.8.8'],
            'nameserver': ['https://dns.google/dns-query', 'tls://dns.google'],
            'fallback': ['tls://1.1.1.1', 'tcp://8.8.4.4', 'https://dns.opendns.com/dns-query'],
            'fallback-filter': {'geoip': True, 'geoip-code': 'CN', 'ipcidr': ['240.0.0.0/4']}
        },
        'proxies': all_parsed_proxies,
        'proxy-groups': [
            {
                'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'type': 'select',
                'proxies': ['DIRECT'] + ([p['name'] for p in all_parsed_proxies] if all_parsed_proxies else [])
            },
            {
                'name': 'ğŸ“² å›½å¤–åª’ä½“', 'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT'] + ([p['name'] for p in all_parsed_proxies] if all_parsed_proxies else [])
            },
            {
                'name': 'ğŸ¤– AI/ChatGPT', 'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT'] + ([p['name'] for p in all_parsed_proxies] if all_parsed_proxies else [])
            },
            {
                'name': 'ğŸŒ å…¶ä»–æµé‡', 'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT'] + ([p['name'] for p in all_parsed_proxies] if all_parsed_proxies else [])
            },
            {
                'name': 'ğŸŸ æ¼ç½‘ä¹‹é±¼', 'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT'] + ([p['name'] for p in all_parsed_proxies] if all_parsed_proxies else [])
            },
            {
                'name': 'ğŸ›‘ å¹¿å‘Šæ‹¦æˆª', 'type': 'select',
                'proxies': ['REJECT', 'DIRECT']
            },
            {
                'name': 'ğŸ”° Fallback', 'type': 'fallback',
                'proxies': ([p['name'] for p in all_parsed_proxies] if all_parsed_proxies else ['DIRECT']),
                'url': 'http://www.google.com/generate_204', 'interval': 300
            }
        ],
        'rules': [
            'DOMAIN-KEYWORD,openai,ğŸ¤– AI/ChatGPT',
            'DOMAIN-KEYWORD,google,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,youtube,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,netflix,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,github,ğŸŒ å…¶ä»–æµé‡',
            'DOMAIN-SUFFIX,cn,DIRECT',
            'IP-CIDR,172.16.0.0/12,DIRECT,no-resolve',
            'IP-CIDR,192.168.0.0/16,DIRECT,no-resolve',
            'IP-CIDR,10.0.0.0/8,DIRECT,no-resolve',
            'IP-CIDR,127.0.0.1/8,DIRECT,no-resolve',
            'GEOIP,CN,DIRECT,no-resolve',
            'MATCH,ğŸŸ æ¼ç½‘ä¹‹é±¼'
        ]
    }

    if not all_parsed_proxies:
        for group in clash_config['proxy-groups']:
            if group['name'] not in ['ğŸ›‘ å¹¿å‘Šæ‹¦æˆª', 'ğŸ”° Fallback']:
                group['proxies'] = ['DIRECT']
            elif group['name'] == 'ğŸ”° Fallback':
                group['proxies'] = ['DIRECT']

    final_clash_yaml = yaml.dump(clash_config, allow_unicode=True, sort_keys=False, default_flow_style=False, indent=2)
    with open(OUTPUT_YAML_FILE, "w", encoding="utf-8") as f:
        f.write(final_clash_yaml)
    print(f"Clash YAML configuration successfully written to {OUTPUT_YAML_FILE}")

    final_base64_encoded = base64.b64encode(final_clash_yaml.encode('utf-8')).decode('utf-8')
    with open(OUTPUT_BASE64_FILE, "w", encoding="utf-8") as f:
        f.write(final_base64_encoded)
    print(f"Base64 encoded Clash YAML configuration successfully written to {OUTPUT_BASE64_FILE}")
    
    # ç§»é™¤æ›´æ–° GitHub æ–‡ä»¶çš„é€»è¾‘
    # new_url_list_content = "\n".join(sorted(list(set(successful_urls_list))))
    # if new_url_list_content.strip() != url_content.strip():
    # ... (removed GitHub update logic)
    # else:
    #     print("url.txt file content unchanged, no update needed.")

if __name__ == "__main__":
    main()
